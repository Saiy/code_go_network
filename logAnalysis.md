关于大文件分析，如日志文件，整理一些思路如下：

1、原来思考，完全没有考虑压缩的情况，即.gz格式，但实际大部分日志都是压缩后的。如果没有压缩，就是想通过mmap来分段。

2、其实不管是压缩或者不压缩，都可以顺序读取，然后处理的时候是并发的。思路如下：


 压缩 可以这样 zcat 0522.log.gz | analysisProgram，
 当然也可以在程序里解压 gzip.NewReader(fr) https://golang.org/pkg/compress/gzip/#NewReader
 
 反正都是通过流，顺序处理，而又想并发，所以想到了这个方法：
每读一个大段比如10MB，分割这10MB，分成10个小段，每个小段大概1MB左右，
然后再读一个大段10MB 
然后再读一个大段 10MB
然后再读一个大段 10MB

也有说通过每读一行就去并发的，均可以。

我又考虑到 最好日志分析后，会合并。原先想的是每个1MB小段，还要去找到delimit才做真正的分割。
其实没有必要，完全可以最后合并的时候再去，处理合并上一段的尾巴和下一段的头。
这个时候，如果小段size分的越大，处理合并的量就会越小。

so，这个时候你想到了什么？mapreduce?
